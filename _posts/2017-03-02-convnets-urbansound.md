---
layout: post
title: Audio signal processing and convolutional neural networks
icon: volume-up
---

This project is an example of image classification (more specifically, the techniques associated with image classification). The [UrbanSound dataset](http://www.telegraph.co.uk/travel/destinations/europe/united-kingdom/england/london/articles/Which-foreign-passports-are-most-common-in-London/) is available at that link, and is a labelled collection of recordings of ten different typical city sounds (dogs barking, emergency vehicle sirens, children playing...), and attempted classification of these sounds.

## Audio Processing

There are a few steps to this particular classification problem - to start, let's tackle the problem of getting the sound in a format that is useful for computers. We need to convert what is effectively an analog signal into its digital counterpart to feed into some sort of machine learning algorithm for classification. The signal is usually broken up into small (overlapping) chunks (this is called sampling) - and then the chunks are taken from the time domain into the frequency domain by applying a mathematical transformation called a *Fourier transform*. this transformation produces a vector of complex numbers. These numbers can be stacked against each other vertically, to produce an image of the audio signal - with time along the *x*-axis, frequency along the *y*-axis and intensity of sound represented by the colour at that point. This is known as an audio spectrogram.

## Neural networks ...

So - we've converted an audio classification problem into an image classification problem. Image classification is a hot topic right now - massive tech companies are devoting a lot of time and resources to improving their image recognition technology (facial recognition and computer vision, as examples). Let's approach this task using some similar techniques, then.

Let's start off by talking about neural networks. The classical feed-forward neural network is a technique that has been around for many years, going through peaks and troughs of popularity, as different techniques are discovered, and different variations of neural networks are used. The feed-forward neural network can loosely be thought of as an analogy to the human brain - it's made up of layers of *neurons* and *synapses* (specifically, an *input layer*, where the observed data is fed into the system, a number of *hidden layers*, where these data are processed incrementally, and an *output layer*, where the data are labelled according to which class they're most likely to be a part of)  connecting the neurons together between different layers. Each neuron has attached to it an *activation function* - this can be something like a logistic function, hyperbolic tangent, rectified linear unit - each different activation function will produce different results within the neural network. Each synapse connects a neuron to a neuron in the next layer, and has attached to it a weight. The data is fed into the network, the activation function is applied to each data point at that node, and then the results of this are summated and travel across the synapses connecting that neuron to the ones in the next layer, where they're multiplied by the relevant weights (according to the synapse they're travelling on). This is repeated until we reach the output layer.

## ... to Convolutional Neural Networks

Convolutional neural networks (or convnets, or CNNs for brevity) are an extension of this idea, that have found success in image classification applications. If we consider a digital image, naturally made up of three channels, or layers of pixels (being red, green and blue), then a CNN examines a small area of this image, applies an activation function to the pixels in this area, then sums these numbers and passes them to the next layer, which will usually have more channels/layers that represent more abstract characteristics of the image (for example, image edges or curves). CNNs, then, aren't fully connected - this makes intuitive sense, in that in images the area directly surrounding a pixel will be more relevant to determining what the image is of than area very far from a pixel - and it makes computational sense too, as if the layers were fully connected then the number of calculations we're doing would quickly become untenable.

Convolutional layers are central to the idea of convnets, but a common CNN architecture may consist of a number of different types of layers. A typical modern image recognition CNN would consist of a convolutional layer, followed by a pooling layer, followed by a convolutional layer, followed by a pooling layer... this is repeated several times, each time seeing the dimensions of the data become smaller and deeper (see the image for intuition). The convolved data then needs to be transformed back from its many-layered form to a one-dimensional representation - we apply a number of alternating fully-connected and dropout layers, to reduce the dimensionality and prevent overfitting, respectively, before we finally produce a classification for the image.

## Methodology

Let's have a look at how these techniques apply to this particular example. The audio data is 8732 files of different sounds, each sound being either: an air conditioner, car horn, children playing, dog barking, drill, engine idling, gun shot, jackhammer, siren, or street music. The individual files come in a number of file formats, and come with an accompanying JSON and CSV file, containing relevant metadata. If we listen to a couple of the files to see what we're dealing with - a few dog barks, for example, we can see that some of the sounds have one bark and some have several. For the most part, the CSV files tell us when this is the case - if there's more than one bark in a file then the CSV will tell us how many and when the start and end point of the individual barks are - but this isn't always the case. As well as this, sometimes the sound to be classified is in the foreground of the recording and sometimes it's in the background, and finally the classes are reasonably imbalanced - there are a lot of dogs, but not so many gunshots. There are many techniques available to address class imbalance, and there are several things we could do to address the other issues if they present a problem, but for the moment let's proceed as planned and see how we go.

We can use librosa, which is a common audio processing library within Python created by the [LabROSA team](https://labrosa.ee.columbia.edu/) at Columbia university. The library contains tools to process several different types of audio files (all of the ones we have in the UrbanSound collection, at least) so we don't have to worry too much about anything there. Starting with the list of filenames, we can reasonably easily define a function to process each of the audio files and store the resultant spectrogram as an integer filename in a folder with that image's classification.

```python

```
